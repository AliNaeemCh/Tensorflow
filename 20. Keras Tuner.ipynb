{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner\n",
    "Keras tuner is used to try different models based on different number of neurons per hidden layer, different hidden layers, different learning rates, different optimizers etc.\n",
    "\n",
    "In Keras Tuner, `max_trials` controls the number of different model configurations (i.e., hyperparameter combinations) that will be tested during the hyperparameter search process. It does not necessarily test all possible combinations unless `max_trials` is set to the total number of combinations. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **All Possible Combinations**: \n",
    "   - When you define hyperparameter ranges, Keras Tuner will generate all possible combinations based on the defined search space. For instance:\n",
    "     - Neurons: `[8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]` (16 options)\n",
    "     - Optimizers: `[opt1, opt2, opt3]` (3 options)\n",
    "     - Learning rates: `[lr1, lr2, lr3, lr4]` (4 options)\n",
    "   \n",
    "   - Total combinations: `16 * 3 * 4 = 192`.\n",
    "\n",
    "2. **max_trials**:\n",
    "   - **max_trials = 192**: If you set `max_trials` equal to the total number of combinations (192 in this case), Keras Tuner will try all possible combinations.\n",
    "   - **max_trials < 192**: If `max_trials` is less than 192, Keras Tuner will randomly sample `max_trials` combinations from the total search space. For instance, if `max_trials = 100`, it will randomly test 100 different combinations out of the 192 possible ones.\n",
    "   - **max_trials > 192**: If `max_trials` is greater than the total number of combinations, Keras Tuner will only test each combination once (192 in this case) and ignore the extra trials.\n",
    "\n",
    "In practice, you often use a smaller `max_trials` to save time and computational resources, especially if the search space is very large. Random sampling (when `max_trials` is less than the total combinations) can still yield good results because hyperparameter tuning often benefits from exploring diverse configurations rather than exhaustively searching all possible ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/HousingData.csv')\n",
    "df_cleaned = df.dropna()\n",
    "X = df_cleaned.drop(columns=['MEDV'])\n",
    "y = df_cleaned['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "mm  = MinMaxScaler()\n",
    "mm.fit(X_train)\n",
    "X_train = mm.transform(X_train)\n",
    "X_test = mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16374"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_training_examples = X_train.shape[0]\n",
    "total_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_features = X_train.shape[1]\n",
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, input_shape, num_outputs):\n",
    "    # total combinations for 2 choices of layers, 8 choices of neurons per layer and 3 choices of optimizers and 3 choices of lr: 2x8x3x3=144\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=2)):\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int('neuron'+str(i+1),\n",
    "                                                     min_value=16,\n",
    "                                                     max_value=128,\n",
    "                                                     step=16),\n",
    "                                                     activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1 if num_outputs == 2 else num_outputs))\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('adam_learning_rate', [1e-3, 1e-2, 1e-1]))\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=hp.Choice('sgd_learning_rate', [1e-3, 1e-2, 1e-1]))\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=hp.Choice('rmsprop_learning_rate', [1e-3, 1e-2, 1e-1]))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='mse',\n",
    "                metrics=['mean_absolute_percentage_error'])\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=lambda hp: build_model(hp, input_shape=(total_features, ), num_outputs=2),\n",
    "    objective='val_mean_absolute_percentage_error',\n",
    "    max_trials = 144,\n",
    "    executions_per_trial=1,\n",
    "    directory='.',\n",
    "    project_name='my-tuner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = os.path.join('.', 'my-tuner')\n",
    "\n",
    "# Remove the directory if it exists\n",
    "if os.path.exists(full_path):\n",
    "    shutil.rmtree(full_path)\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
